{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9065469,
          "sourceType": "datasetVersion",
          "datasetId": 5467389
        },
        {
          "sourceId": 85838,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 72113,
          "modelId": 97049
        }
      ],
      "dockerImageVersionId": 30748,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "script",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29cb4f8627f74d63829542d8bf67da54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17fb92199ccc467d9b6c80ec46b9f129",
              "IPY_MODEL_54bec0dda97b4fb88efa09d5d4708d81",
              "IPY_MODEL_7021d2feed254089b4c535af927b65f2"
            ],
            "layout": "IPY_MODEL_6c89f0aea4f64136b536ae5925640151"
          }
        },
        "17fb92199ccc467d9b6c80ec46b9f129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a0df0404d074869bec119aa2f55517a",
            "placeholder": "​",
            "style": "IPY_MODEL_f88a5ddb36684350987583b4787723da",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "54bec0dda97b4fb88efa09d5d4708d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed1c50c6ee5439f8c00207f80a71bff",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11720e09114740058ffe906feea7e133",
            "value": 4
          }
        },
        "7021d2feed254089b4c535af927b65f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0f2d8c298074ba19e7f4e282468e5e0",
            "placeholder": "​",
            "style": "IPY_MODEL_a67741a4fd8a4e7d92d7bc0ed40e0dcf",
            "value": " 4/4 [01:06&lt;00:00, 14.39s/it]"
          }
        },
        "6c89f0aea4f64136b536ae5925640151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a0df0404d074869bec119aa2f55517a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f88a5ddb36684350987583b4787723da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed1c50c6ee5439f8c00207f80a71bff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11720e09114740058ffe906feea7e133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0f2d8c298074ba19e7f4e282468e5e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a67741a4fd8a4e7d92d7bc0ed40e0dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai python-dotenv aiogram transformers torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jfqIemlnGN1d",
        "outputId": "dbc0bc2f-8e5c-4ac9-aa57-8ef9f260b907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.37.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: aiogram in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiofiles~=23.2.1 in /usr/local/lib/python3.10/dist-packages (from aiogram) (23.2.1)\n",
            "Requirement already satisfied: aiohttp~=3.9.0 in /usr/local/lib/python3.10/dist-packages (from aiogram) (3.9.5)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from aiogram) (2024.7.4)\n",
            "Requirement already satisfied: magic-filter<1.1,>=1.0.12 in /usr/local/lib/python3.10/dist-packages (from aiogram) (1.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import asyncio\n",
        "from typing import Optional\n",
        "from aiogram import Bot, Dispatcher, Router, F\n",
        "from aiogram.filters import Command\n",
        "from aiogram.types import Message\n",
        "from aiogram.fsm.context import FSMContext\n",
        "from aiogram.fsm.state import State, StatesGroup\n",
        "from aiogram.fsm.storage.memory import MemoryStorage\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "# from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "\n",
        "# load_dotenv()\n",
        "\n",
        "router = Router()\n",
        "\n",
        "os.environ['NO_GCE_CHECK'] = 'true'\n",
        "\n",
        "login(userdata.get('huggingface_token'))\n",
        "\n",
        "# Bot token\n",
        "BOT_TOKEN = userdata.get('TELEGRAM_TOKEN')\n",
        "\n",
        "# OpenAI setup\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# Model setup\n",
        "MEDICAL_ASSISTANT_MODEL = 'McSimoff/llama-3-8b-medical-assistant-v3'\n",
        "MEDICAL_ASSISTANT_MODEL_K = '/kaggle/input/llama-3-8b-medical-assistant-v3.5/transformers/default/1/llama-3-8b-medical-assistant-v3'\n",
        "DEVICE = \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MEDICAL_ASSISTANT_MODEL\n",
        ").to(DEVICE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MEDICAL_ASSISTANT_MODEL)"
      ],
      "metadata": {
        "_uuid": "54eaffe7-8ab1-4cdb-8d7b-0fb0852a13cc",
        "_cell_guid": "723983b8-5a8b-493c-9cee-dd0c87947dd5",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-07-31T09:16:38.076091Z",
          "iopub.execute_input": "2024-07-31T09:16:38.076493Z"
        },
        "trusted": true,
        "id": "__Ab7xTiFqqr",
        "outputId": "9b7c370a-4da9-4032-bbdc-490d96357e8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "29cb4f8627f74d63829542d8bf67da54",
            "17fb92199ccc467d9b6c80ec46b9f129",
            "54bec0dda97b4fb88efa09d5d4708d81",
            "7021d2feed254089b4c535af927b65f2",
            "6c89f0aea4f64136b536ae5925640151",
            "1a0df0404d074869bec119aa2f55517a",
            "f88a5ddb36684350987583b4787723da",
            "bed1c50c6ee5439f8c00207f80a71bff",
            "11720e09114740058ffe906feea7e133",
            "e0f2d8c298074ba19e7f4e282468e5e0",
            "a67741a4fd8a4e7d92d7bc0ed40e0dcf"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29cb4f8627f74d63829542d8bf67da54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BotStates(StatesGroup):\n",
        "    WAITING_FOR_LEAFLET = State()\n",
        "    READY_FOR_QUESTIONS = State()\n",
        "\n",
        "class LeafletBot:\n",
        "    def __init__(self, model, tokenizer, client):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.client = client\n",
        "        self.chat_history = []\n",
        "        self.pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            torch_dtype=torch.float16,\n",
        "            device=DEVICE,\n",
        "        )\n",
        "\n",
        "    async def process_leaflet(self, image_path: str) -> str:\n",
        "        with open(image_path, 'rb') as image_file:\n",
        "            image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model='gpt-4o-mini',\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"Extract the text from this image of a medicine leaflet.\"},\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=10000,\n",
        "        )\n",
        "\n",
        "        extracted_text = response.choices[0].message.content\n",
        "        self.chat_history = [{\"role\": \"system\", \"content\": f\"Leaflet content: {extracted_text}\"}]\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "                You are an experienced medical doctor. I need to understand how to use medicine. I don't need information\n",
        "                about the leaflet, manufacturer or their contacts, include only information about the medicine in your response.\n",
        "                Provide me a summary of this medicine leaflet in under 250 words,\n",
        "                including name, uses, from 3 to 6 most important side effects, and recommendations for taking the medicine -\n",
        "                in the morning or in the evening, before or after eating, interaction with other medicines or alcohol::\n",
        "                {extracted_text}\n",
        "            \"\"\"\n",
        "        }]\n",
        "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        outputs = self.pipe(prompt, max_new_tokens=350, do_sample=True, temperature=0.5, top_k=50, top_p=0.95)\n",
        "\n",
        "        summary = outputs[0][\"generated_text\"]\n",
        "        assistant_response = summary.split('<|im_start|>assistant\\n')[1].split('<|im_end|>')[0].strip()\n",
        "\n",
        "\n",
        "        return assistant_response\n",
        "\n",
        "    async def process_question(self, question: str) -> str:\n",
        "        messages = self.chat_history + [{\"role\": \"user\", \"content\": \"\"\"\n",
        "            You are an experienced medical doctor. I have sent you a medicine leaflet.\n",
        "            I need to undertand how to take the medicine. I have a question about the medicine from the leaflet.\n",
        "            Answer the question using information from the leaflet. If you don't know the answer, say that I should ask a doctor.\n",
        "            Don't include information about the leaflet or manufacturer, only the answer to the question. Be short, but helpful. Question:\n",
        "            \"\"\" + question}]\n",
        "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        outputs = self.pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.5, top_k=50, top_p=0.95)\n",
        "        answer = outputs[0][\"generated_text\"]\n",
        "#        self.chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "        assitant_answer = answer.split('<|im_start|>assistant\\n')[1].split('<|im_end|>')[0].strip()\n",
        "        return assitant_answer\n"
      ],
      "metadata": {
        "id": "MNgicsLJHW1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize bot and dispatcher\n",
        "bot = Bot(token=BOT_TOKEN)\n",
        "dp = Dispatcher(storage=MemoryStorage())\n",
        "router = Router()\n",
        "\n",
        "# Initialize LeafletBot\n",
        "leaflet_bot = LeafletBot(model, tokenizer, client)\n",
        "\n",
        "@router.message(Command(commands=['start', 'help']))\n",
        "async def send_welcome(message: Message, state: FSMContext):\n",
        "    await message.reply(\"Welcome! Please send me a photo of a medicine leaflet to get started.\")\n",
        "    await state.set_state(BotStates.WAITING_FOR_LEAFLET)\n",
        "\n",
        "@router.message(BotStates.WAITING_FOR_LEAFLET, F.content_type.in_({'image', 'photo'}))\n",
        "async def handle_leaflet(message: Message, state: FSMContext):\n",
        "    # Download the photo\n",
        "    photo = message.photo[-1]\n",
        "    file_id = photo.file_id\n",
        "    file = await bot.get_file(file_id)\n",
        "    file_path = file.file_path\n",
        "\n",
        "    local_filename = f\"leaflet_{message.from_user.id}.jpg\"\n",
        "    await bot.download_file(file_path, local_filename)\n",
        "\n",
        "    # Process the leaflet\n",
        "    await message.reply(\"Processing the leaflet. This may take a moment...\")\n",
        "    try:\n",
        "        summary = await leaflet_bot.process_leaflet(local_filename)\n",
        "        await message.reply(\"Here's a summary of the most important information from the leaflet:\")\n",
        "        await message.reply(summary)\n",
        "        await message.reply(\"You can now ask questions about this medicine.\")\n",
        "        await state.set_state(BotStates.READY_FOR_QUESTIONS)\n",
        "    except Exception as e:\n",
        "        await message.reply(f\"An error occurred while processing the leaflet: {str(e)}\")\n",
        "        await state.set_state(BotStates.WAITING_FOR_LEAFLET)\n",
        "\n",
        "@router.message(BotStates.READY_FOR_QUESTIONS)\n",
        "async def handle_question(message: Message, state: FSMContext):\n",
        "    question = message.text\n",
        "    await message.reply(\"Processing your question. This may take a moment...\")\n",
        "    response = await leaflet_bot.process_question(question)\n",
        "    await message.reply(response)\n",
        "\n",
        "# Add the router to the dispatcher\n",
        "dp.include_router(router)"
      ],
      "metadata": {
        "id": "Co1c0DNuHXJm",
        "outputId": "c0ff4005-3b25-4ec0-bb4f-1aa504f9fa62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Router '0x7bc396161510'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await dp.start_polling(bot)"
      ],
      "metadata": {
        "id": "y_0oU_c8H3p1",
        "outputId": "3fc3c6c7-8648-439a-9666-36d89dc2a34d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:aiogram.dispatcher:Failed to fetch updates - TelegramNetworkError: HTTP Client says - Request timeout error\n",
            "WARNING:aiogram.dispatcher:Sleep for 1.000000 seconds and try again... (tryings = 0, bot id = 7165912989)\n",
            "ERROR:aiogram.dispatcher:Failed to fetch updates - TelegramNetworkError: HTTP Client says - Request timeout error\n",
            "WARNING:aiogram.dispatcher:Sleep for 1.000000 seconds and try again... (tryings = 0, bot id = 7165912989)\n",
            "ERROR:aiogram.dispatcher:Failed to fetch updates - TelegramNetworkError: HTTP Client says - Request timeout error\n",
            "WARNING:aiogram.dispatcher:Sleep for 1.000000 seconds and try again... (tryings = 0, bot id = 7165912989)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_mOQElyYH_dM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}